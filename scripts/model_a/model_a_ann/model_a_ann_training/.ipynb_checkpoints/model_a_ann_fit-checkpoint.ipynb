{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1314772-f552-4cc6-9c2a-a2b6b56c284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "# from keras.metrics import BinaryAccuracy, AUC, BinaryCrossentropy\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf9ac4-36af-4575-baf6-6fb382429fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_a_training = os.path.join(data_dir, 'working_data/model_a/model_a_training')\n",
    "dir_working_model_a_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e82b6-0775-49fc-a6c5-4593f096af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x_1_a_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/x_1_a.parquet')\n",
    "fn_y_fit_1_a_ann = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_1_a_ann.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0256714-d5e0-446f-ad0e-eed57eea252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x = os.path.join(dir_working_model_a_training, 'x.parquet')\n",
    "fn_y = os.path.join(dir_working_model_a_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_a_training, 'id.parquet')\n",
    "\n",
    "fn_model = os.path.join(dir_working_model_a_training, 'model_a_ann.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec06f5-0908-429f-976a-723403452726",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_params = os.path.join(dir_working_model_a_training, 'model_a_ann_hp.csv')\n",
    "params = pd.read_csv(fn_params).to_dict(orient='list')\n",
    "params = {k:params[k][0] for k in params.keys()}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd296791-e5b1-4306-87f0-c3b08706e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd76000-134c-409d-944b-0fc9c5aba83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(fn_x)\n",
    "Y = pd.read_parquet(fn_y)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480c804-69d7-4121-8a2f-d20fff85a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all done automagically by the R script that creates the new data tranches.\n",
    "# We only need to do this for the final model training\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "XClean = standard_scaler.transform(X)\n",
    "XClean = np_cleaning(XClean)\n",
    "\n",
    "pd.DataFrame(XClean).to_parquet(fn_x_1_a_out)\n",
    "\n",
    "XClean = convert_to_tensor(XClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefb952-ba3c-4460-8ee1-62926ec8b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(rate=params[\"dropout_1\"]))\n",
    "model.add(layers.Dense(units=int(params[\"relu_1\"]), activation='relu'))    \n",
    "model.add(layers.Dropout(rate=params[\"dropout_2\"]))\n",
    "model.add(layers.Dense(units=int(params[\"relu_2\"]), activation='relu'))   \n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.BinaryCrossentropy(),\n",
    "        metrics.BinaryAccuracy(), \n",
    "        metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    XClean, Y, epochs=int(params['epochs']), batch_size=128,  # hard-coded here\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72de505-e017-4edb-b7ad-5fd86620fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb99213-fdfb-4a5a-b406-48a7b1f169a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = model.predict(XClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f82fa9-987a-4c6f-b760-410434667865",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_fit).rename(columns={0:'y_fit_1_a_ann'}).to_parquet(fn_y_fit_1_a_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe44c436-afcb-4e85-8495-7d385b92010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook model_a_ann_fit.ipynb to script\n",
      "[NbConvertApp] Writing 2441 bytes to model_a_ann_fit.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script model_a_ann_fit.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
