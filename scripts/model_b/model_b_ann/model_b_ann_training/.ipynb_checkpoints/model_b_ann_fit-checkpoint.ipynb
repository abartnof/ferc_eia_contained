{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1314772-f552-4cc6-9c2a-a2b6b56c284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "# from keras.metrics import BinaryAccuracy, AUC, BinaryCrossentropy\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cf9ac4-36af-4575-baf6-6fb382429fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Extreme SSD/rematch_eia_ferc1_docker/working_data/model_b/model_b_training'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')\n",
    "dir_working_model_b_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0256714-d5e0-446f-ad0e-eed57eea252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y = os.path.join(dir_working_model_b_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_b_training, 'id.parquet')\n",
    "\n",
    "fn_model = os.path.join(dir_working_model_b_training, 'model_b_ann.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd296791-e5b1-4306-87f0-c3b08706e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03f067b-5200-48b4-b5e8-9686470f0ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_1': 0.0177167990488144, 'dropout_2': 0.0059530801109855, 'relu_1': 56, 'relu_2': 29, 'epochs': 14}\n"
     ]
    }
   ],
   "source": [
    "fn_params = os.path.join(dir_working_model_b_training, 'model_b_ann_hp.csv')\n",
    "params = pd.read_csv(fn_params).to_dict(orient='list')\n",
    "params = {k:params[k][0] for k in params.keys()}\n",
    "\n",
    "# params['metrics'] = ['binary_logloss', 'auc']\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33ded7a-8fba-4e6f-90da-1ef7c22456c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'dropout_1': 0.0177,\n",
    "#     'dropout_2': 0.00595,\n",
    "#     'relu_1': 56,\n",
    "#     'relu_2': 29,\n",
    "#     'epochs': 14\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd76000-134c-409d-944b-0fc9c5aba83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(fn_x)\n",
    "Y = pd.read_parquet(fn_y)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7480c804-69d7-4121-8a2f-d20fff85a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all done automagically by the R script that creates the new data tranches.\n",
    "# We only need to do this for the final model training\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "XClean = standard_scaler.transform(X)\n",
    "XClean = np_cleaning(XClean)\n",
    "XClean = convert_to_tensor(XClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eefb952-ba3c-4460-8ee1-62926ec8b346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 506us/step - auc: 0.8852 - binary_accuracy: 0.9992 - binary_crossentropy: 0.0046 - loss: 0.0046\n",
      "Epoch 2/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 456us/step - auc: 0.9095 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 3/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 478us/step - auc: 0.9052 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 4/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 499us/step - auc: 0.9131 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 490us/step - auc: 0.9145 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 6/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 475us/step - auc: 0.9112 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 7/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 472us/step - auc: 0.9073 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 8/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 466us/step - auc: 0.9093 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 9/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 473us/step - auc: 0.9163 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 10/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 473us/step - auc: 0.9102 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 11/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 466us/step - auc: 0.9098 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "Epoch 12/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 484us/step - auc: 0.9075 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0041 - loss: 0.0041\n",
      "Epoch 13/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 496us/step - auc: 0.9065 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0044 - loss: 0.0044\n",
      "Epoch 14/14\n",
      "\u001b[1m49800/49800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 487us/step - auc: 0.9097 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0044 - loss: 0.0044\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(rate=params[\"dropout_1\"]))\n",
    "model.add(layers.Dense(units=int(params[\"relu_1\"]), activation='relu'))    \n",
    "model.add(layers.Dropout(rate=params[\"dropout_2\"]))\n",
    "model.add(layers.Dense(units=int(params[\"relu_2\"]), activation='relu'))   \n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.BinaryCrossentropy(),\n",
    "        metrics.BinaryAccuracy(), \n",
    "        metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    XClean, Y, epochs=int(params['epochs']), batch_size=128,  # hard-coded here\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72de505-e017-4edb-b7ad-5fd86620fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe44c436-afcb-4e85-8495-7d385b92010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook model_b_ann_fit.ipynb to script\n",
      "[NbConvertApp] Writing 2329 bytes to model_b_ann_fit.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script model_b_ann_fit.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
