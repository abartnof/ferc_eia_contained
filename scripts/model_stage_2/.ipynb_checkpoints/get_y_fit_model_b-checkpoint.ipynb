{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e4028-f66d-47ff-a9c0-926cb5b61a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c5db0-9921-4588-8662-f2f127c1b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d3ae7a-2beb-4df0-acc7-d63141bd3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')\n",
    "# dir_working_model_b_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3016c808-53b4-4418-8a92-8324af3baf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file\n",
    "fn_y_fit_model_b = os.path.join(data_dir, 'working_data/model_z/y_fit_model_b.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cb374b-4844-4a39-81ac-29e646a0761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y = os.path.join(dir_working_model_b_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_b_training, 'id.parquet')\n",
    "\n",
    "X = pd.read_parquet(fn_x)\n",
    "Y = pd.read_parquet(fn_y)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ec8a73-c802-4cf0-875b-2da8fed9b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model_b_ann = {\n",
    "    'dropout_1': 0.0177,\n",
    "    'dropout_2': 0.00595,\n",
    "    'relu_1': 56,\n",
    "    'relu_2': 29,\n",
    "    'epochs': 14\n",
    "}\n",
    "params_model_b_gbm = {\n",
    "    'num_trees':266,\n",
    "    'learning_rate':0.0105,\n",
    "    'min_data_in_leaf':42,\n",
    "    'objective':'binary',\n",
    "    'early_stopping_round':-1,\n",
    "    'metrics':['binary_logloss', 'auc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09c27fe-f56c-4290-acb0-b3a568bc1d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b75f0f62c84aca8ae1d7ec25863b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5090, number of negative: 5090000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1502\n",
      "[LightGBM] [Info] Number of data points in the train set: 5095090, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 492us/step - auc: 0.8830 - binary_accuracy: 0.9990 - binary_crossentropy: 0.0054 - loss: 0.0054\n",
      "Epoch 2/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 455us/step - auc: 0.9026 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 3/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.8996 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "Epoch 4/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9022 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9113 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 6/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9097 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 7/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9038 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 8/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.8979 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 9/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8969 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 10/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 464us/step - auc: 0.8993 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 11/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 469us/step - auc: 0.9090 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 12/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 462us/step - auc: 0.9096 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 13/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 464us/step - auc: 0.9070 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 14/14\n",
      "\u001b[1m39806/39806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 465us/step - auc: 0.9098 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "\u001b[1m39978/39978\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 205us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5115, number of negative: 5115000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.344006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 5120115, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 447us/step - auc: 0.8774 - binary_accuracy: 0.9991 - binary_crossentropy: 0.0054 - loss: 0.0054\n",
      "Epoch 2/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.8887 - binary_accuracy: 0.9993 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 3/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 451us/step - auc: 0.8945 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 4/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.9021 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "Epoch 5/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9106 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 6/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 447us/step - auc: 0.9073 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 7/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 457us/step - auc: 0.9091 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 8/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 457us/step - auc: 0.9161 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 9/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 447us/step - auc: 0.9018 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 10/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 447us/step - auc: 0.8977 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 11/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 456us/step - auc: 0.9164 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 12/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 451us/step - auc: 0.9134 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 13/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.9006 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0038 - loss: 0.0038\n",
      "Epoch 14/14\n",
      "\u001b[1m40001/40001\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 451us/step - auc: 0.9052 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "\u001b[1m39196/39196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 198us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5068, number of negative: 5068000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.327798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1533\n",
      "[LightGBM] [Info] Number of data points in the train set: 5073068, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8867 - binary_accuracy: 0.9988 - binary_crossentropy: 0.0056 - loss: 0.0056\n",
      "Epoch 2/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9124 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 3/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9085 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 4/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9080 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 5/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.9158 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 6/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9173 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 7/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9135 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 8/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9077 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 9/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9053 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 10/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.9051 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 11/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9146 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 12/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9159 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 13/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 452us/step - auc: 0.9085 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 14/14\n",
      "\u001b[1m39634/39634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 477us/step - auc: 0.9100 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "\u001b[1m40666/40666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 198us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5137, number of negative: 5137000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1539\n",
      "[LightGBM] [Info] Number of data points in the train set: 5142137, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 455us/step - auc: 0.8910 - binary_accuracy: 0.9984 - binary_crossentropy: 0.0062 - loss: 0.0062\n",
      "Epoch 2/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9157 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 3/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.8981 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 4/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.8941 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 5/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.8884 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 6/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8790 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0040 - loss: 0.0040\n",
      "Epoch 7/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8974 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 8/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9045 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 9/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9018 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "Epoch 10/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.8989 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 11/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9084 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 12/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9024 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0038 - loss: 0.0038\n",
      "Epoch 13/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9074 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 14/14\n",
      "\u001b[1m40173/40173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9005 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "\u001b[1m38508/38508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 200us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5062, number of negative: 5062000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1538\n",
      "[LightGBM] [Info] Number of data points in the train set: 5067062, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.8751 - binary_accuracy: 0.9984 - binary_crossentropy: 0.0063 - loss: 0.0063\n",
      "Epoch 2/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.9052 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 3/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 447us/step - auc: 0.9063 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 4/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9032 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9050 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 6/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.8970 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 7/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 465us/step - auc: 0.8999 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 8/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 455us/step - auc: 0.9045 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 9/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 467us/step - auc: 0.9046 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 10/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9106 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 11/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 456us/step - auc: 0.9067 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 12/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.9035 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 13/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9047 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 14/14\n",
      "\u001b[1m39587/39587\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9059 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "\u001b[1m40854/40854\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 199us/step\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for fold in tqdm(ID.fold.unique()):\n",
    "    \n",
    "    # Create Test and Train subsets based on fold num\n",
    "    is_train_mask = ID.fold != fold\n",
    "    XTrain = X.loc[is_train_mask]\n",
    "    XTest = X.loc[~is_train_mask]\n",
    "    \n",
    "    YTrain = Y.loc[is_train_mask]\n",
    "    YTest = Y.loc[~is_train_mask]\n",
    "    \n",
    "    # Clean the X datasets, based on the training data characteristics\n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler.fit(XTrain)\n",
    "    XTrain = standard_scaler.transform(XTrain)\n",
    "    XTest = standard_scaler.transform(XTest)\n",
    "    \n",
    "    XTrain = np_cleaning(XTrain)\n",
    "    XTest = np_cleaning(XTest)\n",
    "\n",
    "    # Fit models\n",
    "    # GBM\n",
    "    train_set = lgb.Dataset(XTrain, YTrain)\n",
    "    mod_b_gbm = lgb.train(\n",
    "            params = params_model_b_gbm,\n",
    "            train_set=train_set\n",
    "        )\n",
    "\n",
    "    # ANN\n",
    "    clear_session()\n",
    "    mod_b_ann = models.Sequential()\n",
    "    mod_b_ann.add(layers.Dropout(rate=params_model_b_ann[\"dropout_1\"]))\n",
    "    mod_b_ann.add(layers.Dense(units=params_model_b_ann[\"relu_1\"], activation='relu'))    \n",
    "    mod_b_ann.add(layers.Dropout(rate=params_model_b_ann[\"dropout_2\"]))\n",
    "    mod_b_ann.add(layers.Dense(units=params_model_b_ann[\"relu_2\"], activation='relu'))   \n",
    "    mod_b_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    mod_b_ann.compile(\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryCrossentropy(),\n",
    "            metrics.BinaryAccuracy(), \n",
    "            metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    history = mod_b_ann.fit(\n",
    "        XTrain, YTrain, epochs=params_model_b_ann['epochs'], batch_size=128,  # hard-coded here\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    yfit_b_gbm = mod_b_gbm.predict(XTest)\n",
    "    \n",
    "    yfit_b_ann = mod_b_ann.predict(XTest)\n",
    "    yfit_b_ann = yfit_b_ann.reshape(-1,)\n",
    "\n",
    "    # Join ID to YFit, store together\n",
    "    RelevantID = ID.loc[~is_train_mask, ['record_id_ferc1', 'record_id_eia', 'fold']].reset_index(drop=True)\n",
    "\n",
    "    RelevantYFit = pd.DataFrame({\n",
    "        'y_fit_b_ann':yfit_b_ann,\n",
    "        'y_fit_b_gbm':yfit_b_gbm\n",
    "    })\n",
    "    \n",
    "    Results = pd.concat([RelevantID, RelevantYFit], axis=1)\n",
    "    results_list.append(Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c287b1c8-24fe-4aea-b03a-3f4c1ec2af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results_list).reset_index(drop=True).to_parquet(fn_y_fit_model_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
