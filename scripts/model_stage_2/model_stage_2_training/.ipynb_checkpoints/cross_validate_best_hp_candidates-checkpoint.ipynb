{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed35702d-6fe0-4092-8ccd-42d8af8aaa50",
   "metadata": {},
   "source": [
    "# Perform cross-validation on the best options returned by the hyperparameter search\n",
    "\n",
    "__author__: Andrew Bartnof\n",
    "\n",
    "__copyright__: Copyright 2025, Rocky Mountain Institute\n",
    "\n",
    "__credits__: Alex Engel, Andrew Bartnof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ed92e-0b73-43b7-ba46-8186a5ccae3a",
   "metadata": {},
   "source": [
    "Stage 1 hyperparameters:\n",
    "- These are a 'fait accompli', and the hyperparameters need only be loaded.\n",
    "\n",
    "Stage 2, testable hyperparameters:\n",
    "- Load the top n number of possible hyperparameters per iteration.\n",
    "\n",
    "Load the mostly feature-engineered stage 1 X and Y files\n",
    "\n",
    "For each stage 2 hyperparameter x each fold number:\n",
    "- Split the folds into premier_model_fits (x2), secondary_model_train (x2), secondary_model_test (x1)\n",
    "- Normalize the premier_model_fits X files\n",
    "- Train all four input models on the premier_model_fits files\n",
    "- Fit a stage 2 model with the contender stage 2 hyperparameters\n",
    "- Test the loss, accuracy, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38425028-a7df-4608-8624-da6594f34d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84eb410-efd6-46d2-b97c-2c034e490309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_a_training = os.path.join(data_dir, 'working_data/model_a/model_a_training')\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')\n",
    "\n",
    "# fn_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/second_stage_model_cv.csv')\n",
    "dir_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/cv_results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5686f8-7005-4a8d-bc40-2b0afb7adf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most promising stage 2 hyperparameters from results of grid search\n",
    "\n",
    "fn_hp2 = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/gbm_raytune_2025_03_21/gbm_grid_2025_03_21.csv')\n",
    "HP2 = pd.read_csv(fn_hp2)\n",
    "HP2 = HP2.loc[ HP2['rank'] < 5 ]\n",
    "HP2 = HP2.set_index('rank', drop=True)\n",
    "HP2 = HP2[['config/verbose', 'config/num_trees', 'config/learning_rate', 'config/min_data_in_leaf', 'config/objective', 'config/early_stopping_round', 'config/metrics']]\n",
    "HP2 = HP2.rename(columns=lambda x: re.sub('^config/','',x))\n",
    "HP2 = HP2.merge( pd.DataFrame({'fold_num':np.arange(5)}), how='cross')\n",
    "\n",
    "hp2_dict = HP2.to_dict(orient='index')\n",
    "\n",
    "for k in hp2_dict.keys():\n",
    "    # Metrics\n",
    "    hp2_dict[k]['metrics'] = ['binary_logloss', 'auc']\n",
    "    # fn_out\n",
    "    hp2_dict[k]['fn_out'] = 'num_trees_' + str(hp2_dict[k]['num_trees']) + '__learning_rate_' + str(hp2_dict[k]['learning_rate']) + '__min_data_in_leaf_' + str(hp2_dict[k]['min_data_in_leaf']) + '__fold_num_' + str(hp2_dict[k]['fold_num'])\n",
    "    hp2_dict[k]['fn_out'] = re.sub(r'\\.', 'DOT', hp2_dict[k]['fn_out'])\n",
    "    hp2_dict[k]['fn_out'] = hp2_dict[k]['fn_out'] + '.csv'\n",
    "    hp2_dict[k]['fn_out'] = os.path.join(dir_out, hp2_dict[k]['fn_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b97319-d3d3-448f-b95e-dff47f260045",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x_a = os.path.join(dir_working_model_a_training, 'x.parquet')\n",
    "fn_y_a = os.path.join(dir_working_model_a_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_a_training, 'id.parquet')\n",
    "\n",
    "fn_x_b = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y_b = os.path.join(dir_working_model_b_training, 'y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d201b9f5-b777-4db9-aaf4-44d551ad63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X\n",
    "\n",
    "def prep_x(fit_scaler, X):\n",
    "    X = fit_scaler.transform(X)\n",
    "    X = np_cleaning(X)\n",
    "    # X = convert_to_tensor(X)\n",
    "    return(X)\n",
    "\n",
    "def get_grouped_rank(ID, mask, y_fit):\n",
    "    # Return an array with each y_fit's rank, in descending order + dense method, grouped \n",
    "    # by record_id_ferc1\n",
    "    CteTrain = ID.loc[mask, ['record_id_ferc1']].copy()\n",
    "    # CteTrain = ID.loc[is_secondary_model_train, ['record_id_ferc1']].copy()\n",
    "    CteTrain['y_fit'] = y_fit\n",
    "    # CteTrain['y_fit'] = y_fit_train_a_gbm\n",
    "    CteTrain.groupby('record_id_ferc1')['y_fit'].rank(method='dense', ascending=False)\n",
    "    return CteTrain[['y_fit']]\n",
    "# Cte[['y_fit']] = y_fit_train\n",
    "\n",
    "def define_folds(values_for_secondary_model_test):\n",
    "    # If fold f is reserved for secondary model_test, split the remaining folds\n",
    "    # half into premier_model_fits, and half into secondary_model_train. \n",
    "    # Return all values, as numpy arrays\n",
    "    fold_values = np.arange(5)\n",
    "    remaining_fold_values = np.setdiff1d(fold_values, values_for_secondary_model_test)\n",
    "    np.random.shuffle(remaining_fold_values)\n",
    "    values_for_premier_model_fits = remaining_fold_values[0:2]\n",
    "    values_for_secondary_model_train = remaining_fold_values[2:]\n",
    "    \n",
    "    print('Values for premier model fits: ', values_for_premier_model_fits)\n",
    "    print('Values for secondary model train: ', values_for_secondary_model_train)\n",
    "    print('Values for secondary model test: ', values_for_secondary_model_test)\n",
    "    \n",
    "    return values_for_premier_model_fits, values_for_secondary_model_train, np.array(values_for_secondary_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba969e9-6836-4a56-bf41-2119ab3ea7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boolean_masks_for_folds(ID, values_for_premier_model_fits, values_for_secondary_model_train, values_for_secondary_model_test):\n",
    "    is_premier_model_fits    = np.isin(element=ID.fold.values, test_elements=values_for_premier_model_fits)\n",
    "    is_secondary_model_train = np.isin(element=ID.fold.values, test_elements=values_for_secondary_model_train)\n",
    "    is_secondary_model_test  = np.isin(element=ID.fold.values, test_elements=values_for_secondary_model_test)\n",
    "    return is_premier_model_fits, is_secondary_model_train, is_secondary_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7a1fb2-c8bd-4b51-aef4-d9b1c545aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ann(params, X, Y):\n",
    "    # Can be used by model A or B\n",
    "    clear_session()\n",
    "    model_ann = models.Sequential()\n",
    "    model_ann.add(layers.Dropout(rate=params[\"dropout_1\"]))\n",
    "    model_ann.add(layers.Dense(units=int(params[\"relu_1\"]), activation='relu'))    \n",
    "    model_ann.add(layers.Dropout(rate=params[\"dropout_2\"]))\n",
    "    model_ann.add(layers.Dense(units=int(params[\"relu_2\"]), activation='relu'))   \n",
    "    model_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model_ann.compile(\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryCrossentropy(),\n",
    "            metrics.BinaryAccuracy(), \n",
    "            metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    history_ann = model_ann.fit(\n",
    "        convert_to_tensor(X), Y, epochs=int(params['epochs']), batch_size=128,\n",
    "        verbose=0\n",
    "    ) \n",
    "    return model_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976ed19a-94fd-43c4-850e-12bbb0e2ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gbm(params, X, Y):\n",
    "    # Can be used by model A or B\n",
    "    train_set = lgb.Dataset(X, Y)\n",
    "    model_gbm = lgb.train(\n",
    "            params = params,\n",
    "            train_set=train_set   \n",
    "        )\n",
    "    return model_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1dc0c60-ef94-4680-a6e0-daa0570e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_x(X, is_premier_model_fits, is_secondary_model_train, is_secondary_model_test):\n",
    "    # Scale X files \n",
    "    XPremierModelFits    = X.loc[is_premier_model_fits]\n",
    "    XSecondaryModelTrain = X.loc[is_secondary_model_train]\n",
    "    XSecondaryModelTest  = X.loc[is_secondary_model_test]\n",
    "    \n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler = standard_scaler.fit(XPremierModelFits)\n",
    "    \n",
    "    XPremierModelFits    = prep_x(fit_scaler=standard_scaler, X=XPremierModelFits)\n",
    "    XSecondaryModelTrain = prep_x(fit_scaler=standard_scaler, X=XSecondaryModelTrain)\n",
    "    XSecondaryModelTest  = prep_x(fit_scaler=standard_scaler, X=XSecondaryModelTest)\n",
    "    return XPremierModelFits, XSecondaryModelTrain, XSecondaryModelTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53275e9a-5dfe-4032-92a1-157b41faef3c",
   "metadata": {},
   "source": [
    "# Collect Stage 1 Hyperparameters\n",
    "\n",
    "Note that these are a 'fait accompli', and need only be read from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d068f8-aa6b-4ebf-bc26-8a284ea0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_a_ann_hp = os.path.join(data_dir, 'working_data/model_a/model_a_training/model_a_ann_hp.csv')\n",
    "hp1_a_ann = pd.read_csv(fn_model_a_ann_hp).to_dict(orient='list')\n",
    "hp1_a_ann = {k:hp1_a_ann[k][0] for k in hp1_a_ann.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eff65ce5-6785-4de2-a2ca-c2c5974a1d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_a_gbm_hp = os.path.join(data_dir, 'working_data/model_a/model_a_training/model_a_gbm_hp.csv')\n",
    "hp1_a_gbm = pd.read_csv(fn_model_a_gbm_hp).to_dict(orient='list')\n",
    "hp1_a_gbm = {k:hp1_a_gbm[k][0] for k in hp1_a_gbm.keys()}\n",
    "hp1_a_gbm['metrics'] = ['binary_logloss', 'auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50bd602-cf97-495c-93d0-87b970d24aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_b_ann_hp = os.path.join(data_dir, 'working_data/model_b/model_b_training/model_b_ann_hp.csv')\n",
    "hp1_b_ann = pd.read_csv(fn_model_b_ann_hp).to_dict(orient='list')\n",
    "hp1_b_ann = {k:hp1_b_ann[k][0] for k in hp1_b_ann.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0295ca10-c14f-478f-9954-094d7ba94c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_b_gbm_hp = os.path.join(data_dir, 'working_data/model_b/model_b_training/model_b_gbm_hp.csv')\n",
    "hp1_b_gbm = pd.read_csv(fn_model_b_gbm_hp).to_dict(orient='list')\n",
    "hp1_b_gbm = {k:hp1_b_gbm[k][0] for k in hp1_b_gbm.keys()}\n",
    "hp1_b_gbm['metrics'] = ['binary_logloss', 'auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66136f3a-f1d6-43f1-b3bc-de85adbdb64f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b1a7e03-bd54-4dd3-8eda-f396546b6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = pd.read_parquet(fn_x_a)\n",
    "X_b = pd.read_parquet(fn_x_b)\n",
    "Y = pd.read_parquet(fn_y_a)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fcaa7-88f3-4795-ad2d-e02a92847738",
   "metadata": {},
   "source": [
    "# Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d48a8d71-5864-483e-9be8-17880ae1d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for premier model fits:  [4 1]\n",
      "Values for secondary model train:  [3 2]\n",
      "Values for secondary model test:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78892/78892\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 202us/step\n",
      "\u001b[1m78892/78892\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 246us/step\n",
      "\u001b[1m38946/38946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201us/step\n",
      "\u001b[1m38946/38946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 205us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: fold_num\n",
      "[LightGBM] [Warning] Unknown parameter: dir_fn_out\n",
      "[LightGBM] [Warning] Unknown parameter: SSD/rematch_eia_ferc1_docker/working_data/model_second_stage/model_second_stage_training/cv_results/0_162_580_0137300158848056.csv\n",
      "[LightGBM] [Warning] Unknown parameter: fold_num\n",
      "[LightGBM] [Warning] Unknown parameter: dir_fn_out\n",
      "[LightGBM] [Warning] Unknown parameter: SSD/rematch_eia_ferc1_docker/working_data/model_second_stage/model_second_stage_training/cv_results/0_162_580_0137300158848056.csv\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.311114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6408\n",
      "[LightGBM] [Info] Number of data points in the train set: 2524522, number of used features: 141\n",
      "[LightGBM] [Info] Start training from score 0.000999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "CV error, moving to next hyperparameters\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(hp2_dict.keys()):\n",
    "    \n",
    "    params = hp2_dict[i]\n",
    "    \n",
    "    does_file_exist =  os.path.isfile( params['fn_out'] )\n",
    "    if not does_file_exist:\n",
    "        try:            \n",
    "            \n",
    "            # Note which fold we're on, and divvy up the data into test/train bits, based on that\n",
    "            values_for_premier_model_fits, values_for_secondary_model_train, values_for_secondary_model_test = define_folds( params['fold_num'] )\n",
    "        \n",
    "            is_premier_model_fits, is_secondary_model_train, is_secondary_model_test = get_boolean_masks_for_folds(\n",
    "                ID=ID, \n",
    "                values_for_premier_model_fits=values_for_premier_model_fits, \n",
    "                values_for_secondary_model_train=values_for_secondary_model_train, \n",
    "                values_for_secondary_model_test=values_for_secondary_model_test\n",
    "            )\n",
    "        \n",
    "            # Y, ID\n",
    "            YPremierModelFits = Y.loc[is_premier_model_fits]\n",
    "            YSecondaryModelTrain = Y.loc[is_secondary_model_train]\n",
    "            YSecondaryModelTest = Y.loc[is_secondary_model_test]\n",
    "            \n",
    "            IDSecondaryModelTest = ID.loc[is_secondary_model_test]\n",
    "        \n",
    "            # XA\n",
    "            XAPremierModelFits, XASecondaryModelTrain, XASecondaryModelTest = clean_x(\n",
    "                X=X_a, \n",
    "                is_premier_model_fits=is_premier_model_fits, \n",
    "                is_secondary_model_train=is_secondary_model_train, \n",
    "                is_secondary_model_test=is_secondary_model_test\n",
    "            )\n",
    "        \n",
    "            # XB\n",
    "            XBPremierModelFits, XBSecondaryModelTrain, XBSecondaryModelTest = clean_x(\n",
    "                X=X_b, \n",
    "                is_premier_model_fits=is_premier_model_fits, \n",
    "                is_secondary_model_train=is_secondary_model_train, \n",
    "                is_secondary_model_test=is_secondary_model_test\n",
    "            )\n",
    "        \n",
    "            # Fit models for stage 1, get y_fit\n",
    "            model_a_ann = fit_ann(params=hp1_a_ann, X=XAPremierModelFits, Y=YPremierModelFits)\n",
    "            model_a_gbm = fit_gbm(params=hp1_a_gbm, X=XAPremierModelFits, Y=YPremierModelFits)\n",
    "            model_b_ann = fit_ann(params=hp1_b_ann, X=XBPremierModelFits, Y=YPremierModelFits)\n",
    "            model_b_gbm = fit_gbm(params=hp1_b_gbm, X=XBPremierModelFits, Y=YPremierModelFits)\n",
    "        \n",
    "            # ANN, stage 2 Train\n",
    "            y_fit_train_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTrain))\n",
    "            y_fit_train_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTrain))\n",
    "        \n",
    "            # ANN, stage 2 Test\n",
    "            y_fit_test_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTest))\n",
    "            y_fit_test_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTest))\n",
    "        \n",
    "            # GBM, stage 2 Train\n",
    "            y_fit_train_a_gbm = model_a_gbm.predict(XASecondaryModelTrain)\n",
    "            y_fit_train_b_gbm = model_b_gbm.predict(XBSecondaryModelTrain)\n",
    "        \n",
    "            # GBM, stage 2 Test\n",
    "            y_fit_test_a_gbm = model_a_gbm.predict(XASecondaryModelTest)\n",
    "            y_fit_test_b_gbm = model_b_gbm.predict(XBSecondaryModelTest)\n",
    "        \n",
    "            # Collect the above into something the 2nd stage model can use\n",
    "            mask=is_secondary_model_train\n",
    "            XSecondaryModelTrain = np.hstack([\n",
    "                XASecondaryModelTrain, \n",
    "                XBSecondaryModelTrain,\n",
    "                \n",
    "                y_fit_train_a_ann,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_train_a_ann).to_numpy(),\n",
    "                \n",
    "                y_fit_train_b_ann,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_train_b_ann).to_numpy(),\n",
    "            \n",
    "            \n",
    "                np.array([y_fit_train_a_gbm]).T,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_train_a_gbm).to_numpy(),\n",
    "            \n",
    "                np.array([y_fit_train_b_gbm]).T,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_train_b_gbm).to_numpy()\n",
    "            ])\n",
    "\n",
    "            mask=is_secondary_model_test           \n",
    "            XSecondaryModelTest = np.hstack([\n",
    "                XASecondaryModelTest, \n",
    "                XBSecondaryModelTest,\n",
    "                \n",
    "                y_fit_test_a_ann,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_test_a_ann).to_numpy(),\n",
    "                \n",
    "                y_fit_test_b_ann,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_test_b_ann).to_numpy(),\n",
    "            \n",
    "                np.array([y_fit_test_a_gbm]).T,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_test_a_gbm).to_numpy(),\n",
    "            \n",
    "                np.array([y_fit_test_b_gbm]).T,\n",
    "                get_grouped_rank(ID=ID, mask=mask, y_fit=y_fit_test_b_gbm).to_numpy()\n",
    "            ])\n",
    "\n",
    "            XTrain = XSecondaryModelTrain\n",
    "            XTest = XSecondaryModelTest\n",
    "            YTrain = YSecondaryModelTrain\n",
    "            YTest = YSecondaryModelTest\n",
    "            \n",
    "            \n",
    "            # Package in training and testing objects\n",
    "            train_set = lgb.Dataset(XTrain, YTrain)\n",
    "            test_set  = lgb.Dataset(XTest,  YTest)\n",
    "            \n",
    "            # Model\n",
    "            gbm = lgb.train(\n",
    "                params,\n",
    "                train_set\n",
    "                # valid_sets=[test_set]    \n",
    "            )\n",
    "            y_fit = gbm.predict(XTest)\n",
    "        \n",
    "            # Goodness of fit\n",
    "            Framework = IDSecondaryModelTest[['record_id_ferc1']].copy()\n",
    "            Framework['y_fit'] = y_fit\n",
    "            Framework['groupwise_max_y_fit'] = Framework.groupby('record_id_ferc1')['y_fit'].transform('max')\n",
    "            Framework['y_fit_adj'] = Framework['y_fit'] == Framework['groupwise_max_y_fit']\n",
    "            \n",
    "            gof_dict = {\n",
    "                'precision' : sklearn_metrics.precision_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "                'recall' : sklearn_metrics.recall_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "                'log_loss' : sklearn_metrics.log_loss(YTest.values, y_fit),\n",
    "                'roc_auc' : sklearn_metrics.roc_auc_score(YTest.values, y_fit)\n",
    "            }\n",
    "\n",
    "            results = params | gof_dict\n",
    "            del results['metrics']\n",
    "            del results['fn_out']\n",
    "            pd.DataFrame(results, index=[0]).to_csv(params['fn_out'], index=False)\n",
    "            \n",
    "        except:\n",
    "            print('CV error, moving to next hyperparameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
