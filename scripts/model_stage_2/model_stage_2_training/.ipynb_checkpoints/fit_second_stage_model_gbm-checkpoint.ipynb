{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e2068d-8512-4126-9630-3f9fc090800d",
   "metadata": {},
   "source": [
    "# Fit the stage 2 model\n",
    "\n",
    "__author__: Andrew Bartnof\n",
    "\n",
    "__copyright__: Copyright 2025, Rocky Mountain Institute\n",
    "\n",
    "__credits__: Alex Engel, Andrew Bartnof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d199-de08-4b5f-9c82-00d31c6f4051",
   "metadata": {},
   "source": [
    "Fit the final model, export model and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f4fe6b-9cff-429a-b452-ea2e869a2321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "import lightgbm as lgb\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758a54bd-899d-4bb9-939d-18529e471c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elt_y_fit(fn_y_fit, ID):\n",
    "    # Load the y_fit, group by record_id_ferc1, return ranks\n",
    "    \n",
    "    YFit = pd.read_parquet(fn_y_fit)\n",
    "    suffix = re.sub('y_fit_', '', YFit.columns[0])\n",
    "    YFit.columns = ['y_fit']\n",
    "    \n",
    "    Cte = ID[['record_id_ferc1']].copy()\n",
    "    Cte = pd.concat([Cte, YFit], axis=1)\n",
    "    Cte['y_fit_rank'] = Cte.groupby('record_id_ferc1')['y_fit'].rank(method='dense', ascending=False)\n",
    "    Cte = Cte[['y_fit', 'y_fit_rank']]\n",
    "    Cte = Cte.rename(columns={'y_fit':'y_fit__' + suffix, 'y_fit_rank':'y_fit_rank__' + suffix})\n",
    "    return Cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262015a4-ba3c-4a83-a21b-987b0d34e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "\n",
    "fn_second_stage_model_gbm_hp = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/model_second_stage_gbm_hp.csv')\n",
    "\n",
    "fn_model_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/model_2.txt')\n",
    "\n",
    "fn_y_fit_2_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_2.parquet')\n",
    "fn_mod2_feature_importance = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/mod2_feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d9fe74-493d-4696-8b87-7dc6cd707255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_trees': 555, 'min_data_in_leaf': 147, 'learning_rate': 0.0143517600943852, 'early_stopping_round': -1, 'metrics': 'binary_logloss'}\n"
     ]
    }
   ],
   "source": [
    "hp = pd.read_csv(fn_second_stage_model_gbm_hp).to_dict('list')\n",
    "hp = {k:hp[k][0] for k in hp.keys()}\n",
    "hp['metrics'] = 'binary_logloss'\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e3fc45-553b-4578-9bd5-987f5d7f900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x_1_a = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/x_1_a.parquet')\n",
    "fn_x_1_b = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/x_1_b.parquet')\n",
    "\n",
    "fn_y_fit_1_a_ann = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_1_a_ann.parquet')\n",
    "fn_y_fit_1_a_gbm = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_1_a_gbm.parquet')\n",
    "fn_y_fit_1_b_ann = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_1_b_ann.parquet')\n",
    "fn_y_fit_1_b_gbm = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/temp/y_fit_1_b_gbm.parquet')\n",
    "\n",
    "fn_id = os.path.join(data_dir, 'working_data/model_a/model_a_training/id.parquet')\n",
    "fn_y = os.path.join(data_dir, 'working_data/model_a/model_a_training/y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ea57d0-27ca-446e-9b93-f5b262abe4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data that doesn't need transformations\n",
    "y = pd.read_parquet(fn_y)\n",
    "ID = pd.read_parquet(fn_id)\n",
    "\n",
    "X1A = pd.read_parquet(fn_x_1_a).reset_index(drop=True)\n",
    "X1B = pd.read_parquet(fn_x_1_b).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf56163-e45e-458e-b55c-6e6dc520b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load y-fit data, adding descending dense ranks to the y_fit\n",
    "YFit1AAnn = elt_y_fit(fn_y_fit=fn_y_fit_1_a_ann, ID=ID).reset_index(drop=True)\n",
    "YFit1AGbm = elt_y_fit(fn_y_fit=fn_y_fit_1_a_gbm, ID=ID).reset_index(drop=True)\n",
    "YFit1BAnn = elt_y_fit(fn_y_fit=fn_y_fit_1_b_ann, ID=ID).reset_index(drop=True)\n",
    "YFit1BGbm = elt_y_fit(fn_y_fit=fn_y_fit_1_b_gbm, ID=ID).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351d745-e116-43df-a878-bc225b841758",
   "metadata": {},
   "source": [
    "The input for this model should look like this:\n",
    "- X encoding A\n",
    "- X encoding B\n",
    "- y-fit, y-fit ranks from ANN A\n",
    "- y-fit, y-fit ranks from GBM A\n",
    "- y-fit, y-fit ranks from ANN B\n",
    "- y-fit, y-fit ranks from GBM B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5585cce-7f45-4859-bd0b-afb81ad51be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.concat([\n",
    "    X1A.add_prefix('x1a_').reset_index(drop=True), \n",
    "    X1B.add_prefix('x1b_').reset_index(drop=True), \n",
    "    YFit1AAnn.reset_index(drop=True), \n",
    "    YFit1AGbm.reset_index(drop=True), \n",
    "    YFit1BAnn.reset_index(drop=True), \n",
    "    YFit1BGbm.reset_index(drop=True)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26cb4677-4fa8-4ac4-a334-bdfd1ad81dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/basic.py:355: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning(\"Converting column-vector to 1d array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.824940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6427\n",
      "[LightGBM] [Info] Number of data points in the train set: 6374368, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 0.000999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x174ef4110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model, save-- without feature names\n",
    "train_data_no_names = lgb.Dataset(X2.values, y.values)\n",
    "mod2_no_names = lgb.train(params=hp, train_set=train_data_no_names)\n",
    "mod2_no_names.save_model(fn_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94399195-3d01-4401-b7bd-9f7d161f0e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.795102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6427\n",
      "[LightGBM] [Info] Number of data points in the train set: 6374368, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 0.000999\n"
     ]
    }
   ],
   "source": [
    "# Extract feature importance, such as they are\n",
    "train_data = lgb.Dataset(X2, y)\n",
    "mod2 = lgb.train(params=hp, train_set=train_data)\n",
    "# mod2.save_model(fn_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b768e740-5a4d-464d-a83b-12843f09a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = mod2.predict(X2)\n",
    "pd.DataFrame(y_fit).rename(columns={0:'y_fit_2'}).to_parquet(fn_y_fit_2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b230867-3f9f-4115-acfe-6e689d5f911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'colnames':X2.columns.to_series(), 'feature_importance':mod2.feature_importance()}).reset_index(drop=True).to_csv(fn_mod2_feature_importance, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e21f8c1-e895-4dfc-8b8b-5aeaa4acc853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook fit_second_stage_model_gbm.ipynb to script\n",
      "[NbConvertApp] Writing 4187 bytes to fit_second_stage_model_gbm.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script fit_second_stage_model_gbm.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
