{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d33fdd-4cc8-42d3-b9a8-85d1b4854190",
   "metadata": {},
   "source": [
    "# Get y-fits from model b ANN and GBM, which will be input for the stage 2 model\n",
    "\n",
    "__author__: Andrew Bartnof\n",
    "\n",
    "__copyright__: Copyright 2025, Rocky Mountain Institute\n",
    "\n",
    "__credits__: Alex Engel, Andrew Bartnof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e4028-f66d-47ff-a9c0-926cb5b61a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c5db0-9921-4588-8662-f2f127c1b967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d3ae7a-2beb-4df0-acc7-d63141bd3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')\n",
    "# dir_working_model_b_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3016c808-53b4-4418-8a92-8324af3baf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file\n",
    "fn_y_fit_model_b = os.path.join(data_dir, 'working_data/model_z/y_fit_model_b.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cb374b-4844-4a39-81ac-29e646a0761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y = os.path.join(dir_working_model_b_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_b_training, 'id.parquet')\n",
    "\n",
    "X = pd.read_parquet(fn_x)\n",
    "Y = pd.read_parquet(fn_y)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9ec8a73-c802-4cf0-875b-2da8fed9b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_model_b_ann = {\n",
    "    'dropout_1': 0.0177,\n",
    "    'dropout_2': 0.00595,\n",
    "    'relu_1': 56,\n",
    "    'relu_2': 29,\n",
    "    'epochs': 14\n",
    "}\n",
    "params_model_b_gbm = {\n",
    "    'num_trees':266,\n",
    "    'learning_rate':0.0105,\n",
    "    'min_data_in_leaf':42,\n",
    "    'objective':'binary',\n",
    "    'early_stopping_round':-1,\n",
    "    'metrics':['binary_logloss', 'auc']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09c27fe-f56c-4290-acb0-b3a568bc1d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5a15135dae4e10b78769b2cb5bbe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5053, number of negative: 5053000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.315044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1535\n",
      "[LightGBM] [Info] Number of data points in the train set: 5058053, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.8875 - binary_accuracy: 0.9986 - binary_crossentropy: 0.0064 - loss: 0.0064\n",
      "Epoch 2/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 458us/step - auc: 0.9016 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 3/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.9020 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 4/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.8925 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 5/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 439us/step - auc: 0.9005 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 6/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 440us/step - auc: 0.9144 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 7/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 441us/step - auc: 0.9126 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 8/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 441us/step - auc: 0.9115 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 9/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 441us/step - auc: 0.9099 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 10/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9134 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 11/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9065 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "Epoch 12/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9009 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 13/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.8999 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 14/14\n",
      "\u001b[1m39517/39517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9137 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "\u001b[1m41135/41135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 200us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5082, number of negative: 5082000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.315960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1530\n",
      "[LightGBM] [Info] Number of data points in the train set: 5087082, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 451us/step - auc: 0.8813 - binary_accuracy: 0.9990 - binary_crossentropy: 0.0056 - loss: 0.0056\n",
      "Epoch 2/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 452us/step - auc: 0.8933 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 3/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.8894 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 4/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 439us/step - auc: 0.9001 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 5/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 439us/step - auc: 0.8910 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0038 - loss: 0.0038\n",
      "Epoch 6/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.8946 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 7/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.8935 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 8/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.9022 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0034 - loss: 0.0034\n",
      "Epoch 9/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.9032 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 10/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 462us/step - auc: 0.9071 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 11/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 483us/step - auc: 0.9024 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 12/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.8956 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 13/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 514us/step - auc: 0.8968 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 14/14\n",
      "\u001b[1m39743/39743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 543us/step - auc: 0.8932 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0040 - loss: 0.0040\n",
      "\u001b[1m40228/40228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 243us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5123, number of negative: 5123000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.424185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1532\n",
      "[LightGBM] [Info] Number of data points in the train set: 5128123, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.8841 - binary_accuracy: 0.9992 - binary_crossentropy: 0.0049 - loss: 0.0049\n",
      "Epoch 2/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 437us/step - auc: 0.8981 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "Epoch 3/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 437us/step - auc: 0.8984 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 4/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 436us/step - auc: 0.9123 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 438us/step - auc: 0.9071 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 6/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.9166 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 7/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.9120 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 8/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 448us/step - auc: 0.8981 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 9/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 452us/step - auc: 0.8963 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "Epoch 10/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 462us/step - auc: 0.9081 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0037 - loss: 0.0037\n",
      "Epoch 11/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.8963 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0043 - loss: 0.0043\n",
      "Epoch 12/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9042 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0043 - loss: 0.0043\n",
      "Epoch 13/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9099 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0040 - loss: 0.0040\n",
      "Epoch 14/14\n",
      "\u001b[1m40064/40064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.8995 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0046 - loss: 0.0046\n",
      "\u001b[1m38946/38946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 199us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5150, number of negative: 5150000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1531\n",
      "[LightGBM] [Info] Number of data points in the train set: 5155150, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 440us/step - auc: 0.8947 - binary_accuracy: 0.9992 - binary_crossentropy: 0.0047 - loss: 0.0047\n",
      "Epoch 2/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.9065 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 3/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.8923 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0032 - loss: 0.0032\n",
      "Epoch 4/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 441us/step - auc: 0.8982 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9099 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 6/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9052 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 7/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9054 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 8/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.9140 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 9/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9136 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0024 - loss: 0.0024\n",
      "Epoch 10/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9093 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 11/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9155 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 12/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9172 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 13/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9106 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 14/14\n",
      "\u001b[1m40275/40275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9135 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "\u001b[1m38101/38101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 200us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5064, number of negative: 5064000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.339116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1531\n",
      "[LightGBM] [Info] Number of data points in the train set: 5069064, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n",
      "Epoch 1/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 449us/step - auc: 0.8843 - binary_accuracy: 0.9991 - binary_crossentropy: 0.0051 - loss: 0.0051\n",
      "Epoch 2/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9056 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 3/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8978 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0030 - loss: 0.0030\n",
      "Epoch 4/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 442us/step - auc: 0.8985 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "Epoch 5/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9159 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 6/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 443us/step - auc: 0.9066 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 7/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9107 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 8/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9054 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 9/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.9026 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0033 - loss: 0.0033\n",
      "Epoch 10/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 444us/step - auc: 0.8985 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0038 - loss: 0.0038\n",
      "Epoch 11/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9021 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0038 - loss: 0.0038\n",
      "Epoch 12/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 446us/step - auc: 0.9053 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 13/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.9010 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 14/14\n",
      "\u001b[1m39603/39603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 445us/step - auc: 0.8943 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0040 - loss: 0.0040\n",
      "\u001b[1m40791/40791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201us/step\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for fold in tqdm(ID.fold.unique()):\n",
    "    \n",
    "    # Create Test and Train subsets based on fold num\n",
    "    is_train_mask = ID.fold != fold\n",
    "    XTrain = X.loc[is_train_mask]\n",
    "    XTest = X.loc[~is_train_mask]\n",
    "    \n",
    "    YTrain = Y.loc[is_train_mask]\n",
    "    YTest = Y.loc[~is_train_mask]\n",
    "    \n",
    "    # Clean the X datasets, based on the training data characteristics\n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler.fit(XTrain)\n",
    "    XTrain = standard_scaler.transform(XTrain)\n",
    "    XTest = standard_scaler.transform(XTest)\n",
    "    \n",
    "    XTrain = np_cleaning(XTrain)\n",
    "    XTest = np_cleaning(XTest)\n",
    "\n",
    "    # Fit models\n",
    "    # GBM\n",
    "    train_set = lgb.Dataset(XTrain, YTrain)\n",
    "    mod_b_gbm = lgb.train(\n",
    "            params = params_model_b_gbm,\n",
    "            train_set=train_set\n",
    "        )\n",
    "\n",
    "    # ANN\n",
    "    clear_session()\n",
    "    mod_b_ann = models.Sequential()\n",
    "    mod_b_ann.add(layers.Dropout(rate=params_model_b_ann[\"dropout_1\"]))\n",
    "    mod_b_ann.add(layers.Dense(units=params_model_b_ann[\"relu_1\"], activation='relu'))    \n",
    "    mod_b_ann.add(layers.Dropout(rate=params_model_b_ann[\"dropout_2\"]))\n",
    "    mod_b_ann.add(layers.Dense(units=params_model_b_ann[\"relu_2\"], activation='relu'))   \n",
    "    mod_b_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    mod_b_ann.compile(\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryCrossentropy(),\n",
    "            metrics.BinaryAccuracy(), \n",
    "            metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    history = mod_b_ann.fit(\n",
    "        XTrain, YTrain, epochs=params_model_b_ann['epochs'], batch_size=128,  # hard-coded here\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    yfit_b_gbm = mod_b_gbm.predict(XTest)\n",
    "    \n",
    "    yfit_b_ann = mod_b_ann.predict(XTest)\n",
    "    yfit_b_ann = yfit_b_ann.reshape(-1,)\n",
    "\n",
    "    # Join ID to YFit, store together\n",
    "    RelevantID = ID.loc[~is_train_mask, ['record_id_ferc1', 'record_id_eia', 'fold']].reset_index(drop=True)\n",
    "\n",
    "    RelevantYFit = pd.DataFrame({\n",
    "        'y_fit_b_ann':yfit_b_ann,\n",
    "        'y_fit_b_gbm':yfit_b_gbm\n",
    "    })\n",
    "    \n",
    "    Results = pd.concat([RelevantID, RelevantYFit], axis=1)\n",
    "    results_list.append(Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c287b1c8-24fe-4aea-b03a-3f4c1ec2af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(results_list).reset_index(drop=True).to_parquet(fn_y_fit_model_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
