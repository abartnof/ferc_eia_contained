{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb935cb3-c5d4-412b-a6b1-3a8523526b11",
   "metadata": {},
   "source": [
    "Steps here:\n",
    "- Subset training data to 2/5 of the folds\n",
    "- fit all four first-order models on this 2/5\n",
    "- Subsequently, we'll need to train a GBM model on 2/5 of the (remaining) folds, and to test on the last 1/5\n",
    "    - These training and testing sets for the second-tier model will need the __feature-engineered__ input X files for each model (A and B), as well as the __y-fit__ for each of the input models (A, B x ANN, GBM). Also, add __descending-rank__ for each prediction, where 1.0 is 1\n",
    "    - It's easier if we pre-process this bit, and save these training and testing sets in a 'temp folder', and then in a second script...\n",
    "      \n",
    "Near future steps:\n",
    "- use raytune, get 250 models\n",
    "- Repeat, using more/less of the original data, as a sensitivity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c0b85d-84a1-435e-84a1-42da33fb38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from ray import train, tune\n",
    "# from ray.tune.search.optuna import OptunaSearch\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c17a1b-7726-437b-80e5-3e3d2d84ce3d",
   "metadata": {},
   "source": [
    "# Establish file locations, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c9c4d4-0579-41d8-8e0e-6dd1bd6b7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_a_training = os.path.join(data_dir, 'working_data/model_a/model_a_training')\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c41a68f-b8f0-4797-8da1-e8bafc3c0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x_a = os.path.join(dir_working_model_a_training, 'x.parquet')\n",
    "fn_y_a = os.path.join(dir_working_model_a_training, 'y.parquet')\n",
    "fn_id_a = os.path.join(dir_working_model_a_training, 'id.parquet')\n",
    "\n",
    "fn_x_b = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y_b = os.path.join(dir_working_model_b_training, 'y.parquet')\n",
    "fn_id_b = os.path.join(dir_working_model_b_training, 'id.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91ae643-7e6d-46a5-9163-091041e44d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (pd.read_parquet(fn_id_a) == pd.read_parquet(fn_id_b)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d760612d-0f3e-4fe4-9a90-b07c4f3609f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_hyperparameter = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/gbm_raytune_2025_02_21')\n",
    "# fn_hyperparameter = os.path.join(dir_hyperparameter, 'gbm_grid_2025_02_21.csv')\n",
    "\n",
    "dir_temp = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/gbm_raytune_2025_02_21/temp_folder')\n",
    "\n",
    "fn_model_a_ann = os.path.join(dir_temp, 'model_a_ann.keras')\n",
    "fn_model_b_ann = os.path.join(dir_temp, 'model_b_ann.keras')\n",
    "fn_model_a_gbm = os.path.join(dir_temp, 'model_a_gbm.txt')\n",
    "fn_model_b_gbm = os.path.join(dir_temp, 'model_b_gbm.txt')\n",
    "\n",
    "fn_x_train = os.path.join(dir_temp, 'x_train.parquet')\n",
    "fn_x_test  = os.path.join(dir_temp, 'x_test.parquet')\n",
    "fn_y_train = os.path.join(dir_temp, 'y_train.parquet')\n",
    "fn_y_test  = os.path.join(dir_temp, 'y_test.parquet')\n",
    "\n",
    "fn_split_characteristics = os.path.join(dir_temp, 'split_characteristics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b567de0-38e2-420d-8768-1f26dad84574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X\n",
    "\n",
    "def prep_x(fit_scaler, X):\n",
    "    X = fit_scaler.transform(X)\n",
    "    X = np_cleaning(X)\n",
    "    # X = convert_to_tensor(X)\n",
    "    return(X)\n",
    "\n",
    "def get_dense_desc_rank(nn):\n",
    "    # will be used for ranking y_fits\n",
    "    return( stats.rankdata(-nn, method='dense') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002c0190-c462-4e1e-a3f6-a3ae16c47131",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_a_ann = {\n",
    "    'dropout_1': 0.000120,\n",
    "    'dropout_2': 0.0633,\n",
    "    'relu_1': 33,\n",
    "    'relu_2': 20,\n",
    "    'epochs': 20\n",
    "}\n",
    "\n",
    "params_a_gbm = {\n",
    "    'num_trees':482,\n",
    "    'learning_rate':0.0134,\n",
    "    'min_data_in_leaf':85,\n",
    "    'objective':'binary',\n",
    "    'early_stopping_round':-1,\n",
    "    'metrics':['binary_logloss', 'auc']\n",
    "}\n",
    "\n",
    "params_b_ann = {\n",
    "    'dropout_1': 0.0177,\n",
    "    'dropout_2': 0.00595,\n",
    "    'relu_1': 56,\n",
    "    'relu_2': 29,\n",
    "    'epochs': 14\n",
    "}\n",
    "\n",
    "params_b_gbm = {\n",
    "    'num_trees':266,\n",
    "    'learning_rate':0.0105,\n",
    "    'min_data_in_leaf':42,\n",
    "    'objective':'binary',\n",
    "    'early_stopping_round':-1,\n",
    "    'metrics':['binary_logloss', 'auc']\n",
    "}\n",
    "\n",
    "params_2_gbm = {\n",
    "    # 'num_iterations': tune.randint(1, 1000),\n",
    "    'verbose':-1,\n",
    "    'num_trees': tune.randint(1, 500),\n",
    "    'learning_rate': tune.uniform(0.0001, 0.75),\n",
    "    'min_data_in_leaf': tune.randint(1, 200),\n",
    "    'objective':'binary', \n",
    "    # 'early_stopping_round':2,\n",
    "    'early_stopping_round':-1,\n",
    "    'metrics':['binary_logloss', 'auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9046704-5abf-4a9b-a6e0-8b27018c6bc8",
   "metadata": {},
   "source": [
    "# Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15bfd3f-80ad-45ad-86d7-09dd85df4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A- data prep\n",
    "\n",
    "X_a = pd.read_parquet(fn_x_a)\n",
    "Y_a = pd.read_parquet(fn_y_a)\n",
    "ID_a = pd.read_parquet(fn_id_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40418995-b8f4-4dee-8dd5-b613ca7e9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premier_model_fits</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>premier_model_fits</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secondary_model_train</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>secondary_model_train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secondary_model_test</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  fold\n",
       "0     premier_model_fits     1\n",
       "1     premier_model_fits     4\n",
       "2  secondary_model_train     3\n",
       "3  secondary_model_train     0\n",
       "4   secondary_model_test     2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 3 folds: fit premier models, train secondary model, test secondary model\n",
    "fold_values = ID_a.fold.unique().tolist()\n",
    "np.random.shuffle(fold_values)\n",
    "values_for_premier_model_fits = fold_values[0:2]\n",
    "values_for_secondary_model_train = fold_values[2:4]\n",
    "values_for_secondary_model_test = fold_values[4]\n",
    "\n",
    "is_premier_model_fits    = np.isin(element=ID_a.fold.values, test_elements=values_for_premier_model_fits)\n",
    "is_secondary_model_test  = np.isin(element=ID_a.fold.values, test_elements=values_for_secondary_model_test)\n",
    "is_secondary_model_train = np.isin(element=ID_a.fold.values, test_elements=values_for_secondary_model_train)\n",
    "\n",
    "frames = [\n",
    "    pd.DataFrame({'name':'premier_model_fits', 'fold':values_for_premier_model_fits}),\n",
    "    pd.DataFrame({'name':'secondary_model_train', 'fold':values_for_secondary_model_train}),\n",
    "    pd.DataFrame({'name':'secondary_model_test', 'fold':[values_for_secondary_model_test]})\n",
    "]\n",
    "SplitCharacteristics = pd.concat(frames, ignore_index=True)\n",
    "SplitCharacteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ad9731-16b8-4e1b-88c8-d9db5b6c6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YPremierModelFits = Y_a.loc[is_premier_model_fits]\n",
    "\n",
    "YSecondaryModelTrain = Y_a.loc[is_secondary_model_train]\n",
    "YSecondaryModelTest = Y_a.loc[is_secondary_model_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4390a3-5219-4c79-87f6-10fb5e9b9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A Premier Model, clean X\n",
    "\n",
    "XAPremierModelFits    = X_a.loc[is_premier_model_fits]\n",
    "XASecondaryModelTrain = X_a.loc[is_secondary_model_train]\n",
    "XASecondaryModelTest  = X_a.loc[is_secondary_model_test]\n",
    "\n",
    "standard_scaler_a = StandardScaler()\n",
    "standard_scaler_a = standard_scaler_a.fit(XAPremierModelFits)\n",
    "\n",
    "XAPremierModelFits    = prep_x(fit_scaler=standard_scaler_a, X=XAPremierModelFits)\n",
    "XASecondaryModelTrain = prep_x(fit_scaler=standard_scaler_a, X=XASecondaryModelTrain)\n",
    "XASecondaryModelTest  = prep_x(fit_scaler=standard_scaler_a, X=XASecondaryModelTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4be3a-aff1-4bba-be4d-8cc0d467b00f",
   "metadata": {},
   "source": [
    "## ANN A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6d3dd1-70ca-47c4-88ea-698c3f4aa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 402us/step - auc: 0.8742 - binary_accuracy: 0.9964 - binary_crossentropy: 0.0111 - loss: 0.0111\n",
      "Epoch 2/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 396us/step - auc: 0.9581 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0018 - loss: 0.0018\n",
      "Epoch 3/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 397us/step - auc: 0.9611 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0017 - loss: 0.0017\n",
      "Epoch 4/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 407us/step - auc: 0.9671 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0015 - loss: 0.0015\n",
      "Epoch 5/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 398us/step - auc: 0.9650 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0015 - loss: 0.0015\n",
      "Epoch 6/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 447us/step - auc: 0.9658 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0015 - loss: 0.0015\n",
      "Epoch 7/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 400us/step - auc: 0.9678 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0015 - loss: 0.0015\n",
      "Epoch 8/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 403us/step - auc: 0.9654 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0017 - loss: 0.0017\n",
      "Epoch 9/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 398us/step - auc: 0.9713 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0015 - loss: 0.0015\n",
      "Epoch 10/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 414us/step - auc: 0.9691 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0013 - loss: 0.0013\n",
      "Epoch 11/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 404us/step - auc: 0.9685 - binary_accuracy: 0.9997 - binary_crossentropy: 0.0016 - loss: 0.0016\n",
      "Epoch 12/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9741 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 13/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9706 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 14/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9731 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 15/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 388us/step - auc: 0.9720 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 16/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9757 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 17/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9739 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 18/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 387us/step - auc: 0.9735 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 19/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9723 - binary_accuracy: 0.9998 - binary_crossentropy: 0.0012 - loss: 0.0012\n",
      "Epoch 20/20\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 386us/step - auc: 0.9798 - binary_accuracy: 0.9998 - binary_crossentropy: 9.7950e-04 - loss: 9.7950e-04\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "model_a_ann = models.Sequential()\n",
    "model_a_ann.add(layers.Dropout(rate=params_a_ann[\"dropout_1\"]))\n",
    "model_a_ann.add(layers.Dense(units=params_a_ann[\"relu_1\"], activation='relu'))    \n",
    "model_a_ann.add(layers.Dropout(rate=params_a_ann[\"dropout_2\"]))\n",
    "model_a_ann.add(layers.Dense(units=params_a_ann[\"relu_2\"], activation='relu'))   \n",
    "model_a_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_a_ann.compile(\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.BinaryCrossentropy(),\n",
    "        metrics.BinaryAccuracy(), \n",
    "        metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "history_a_ann = model_a_ann.fit(\n",
    "    convert_to_tensor(XAPremierModelFits), YPremierModelFits, epochs=params_a_ann['epochs'], batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317527f-f022-45f2-83d5-ecc69970b417",
   "metadata": {},
   "source": [
    "## GBM B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a38062d-2b3a-4fdb-824a-52a2dd7afe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2601, number of negative: 2601000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2849\n",
      "[LightGBM] [Info] Number of data points in the train set: 2603601, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n"
     ]
    }
   ],
   "source": [
    "train_set = lgb.Dataset(XAPremierModelFits, YPremierModelFits)\n",
    "model_a_gbm = lgb.train(\n",
    "        params = params_a_gbm,\n",
    "        train_set=train_set   \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e03bc8-b8cd-45ae-98d3-3314481fe919",
   "metadata": {},
   "source": [
    "# Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252013f6-b083-4b52-a934-7eb2179c4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B- data prep\n",
    "\n",
    "X_b = pd.read_parquet(fn_x_b)\n",
    "Y_b = pd.read_parquet(fn_y_b)\n",
    "# ID_b = pd.read_parquet(fn_id_b)\n",
    "\n",
    "XBPremierModelFits = X_b.loc[is_premier_model_fits]\n",
    "XBSecondaryModelTest = X_b.loc[is_secondary_model_test]\n",
    "XBSecondaryModelTrain = X_b.loc[is_secondary_model_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0af1c1-c42d-4d33-9a6f-ff9c673a17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B Premier Model, clean X\n",
    "\n",
    "XBPremierModelFits    = X_b.loc[is_premier_model_fits]\n",
    "XBSecondaryModelTrain = X_b.loc[is_secondary_model_train]\n",
    "XBSecondaryModelTest  = X_b.loc[is_secondary_model_test]\n",
    "\n",
    "standard_scaler_b = StandardScaler()\n",
    "standard_scaler_b = standard_scaler_b.fit(XBPremierModelFits)\n",
    "\n",
    "XBPremierModelFits    = prep_x(fit_scaler=standard_scaler_b, X=XBPremierModelFits)\n",
    "XBSecondaryModelTrain = prep_x(fit_scaler=standard_scaler_b, X=XBSecondaryModelTrain)\n",
    "XBSecondaryModelTest  = prep_x(fit_scaler=standard_scaler_b, X=XBSecondaryModelTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f3cd2-8cc4-4c68-b787-0c05c9b22f91",
   "metadata": {},
   "source": [
    "## ANN B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac25a41a-3dd8-44e8-909a-bc632b923013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 455us/step - auc: 0.8729 - binary_accuracy: 0.9984 - binary_crossentropy: 0.0076 - loss: 0.0076\n",
      "Epoch 2/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 444us/step - auc: 0.9189 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0026 - loss: 0.0026\n",
      "Epoch 3/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 443us/step - auc: 0.9074 - binary_accuracy: 0.9994 - binary_crossentropy: 0.0028 - loss: 0.0028\n",
      "Epoch 4/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 450us/step - auc: 0.9002 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 5/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 444us/step - auc: 0.8963 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 6/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 457us/step - auc: 0.9096 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 7/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 459us/step - auc: 0.9054 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 8/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 451us/step - auc: 0.9105 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0027 - loss: 0.0027\n",
      "Epoch 9/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 452us/step - auc: 0.9010 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0029 - loss: 0.0029\n",
      "Epoch 10/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 453us/step - auc: 0.9009 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0031 - loss: 0.0031\n",
      "Epoch 11/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 453us/step - auc: 0.8963 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0036 - loss: 0.0036\n",
      "Epoch 12/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 459us/step - auc: 0.8920 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0035 - loss: 0.0035\n",
      "Epoch 13/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 454us/step - auc: 0.8931 - binary_accuracy: 0.9995 - binary_crossentropy: 0.0039 - loss: 0.0039\n",
      "Epoch 14/14\n",
      "\u001b[1m20341/20341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 454us/step - auc: 0.9036 - binary_accuracy: 0.9996 - binary_crossentropy: 0.0033 - loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "model_b_ann = models.Sequential()\n",
    "model_b_ann.add(layers.Dropout(rate=params_b_ann[\"dropout_1\"]))\n",
    "model_b_ann.add(layers.Dense(units=params_b_ann[\"relu_1\"], activation='relu'))    \n",
    "model_b_ann.add(layers.Dropout(rate=params_b_ann[\"dropout_2\"]))\n",
    "model_b_ann.add(layers.Dense(units=params_b_ann[\"relu_2\"], activation='relu'))   \n",
    "model_b_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_b_ann.compile(\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        metrics.BinaryCrossentropy(),\n",
    "        metrics.BinaryAccuracy(), \n",
    "        metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "history_b_ann = model_b_ann.fit(\n",
    "    convert_to_tensor(XBPremierModelFits), YPremierModelFits, epochs=params_b_ann['epochs'], batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac7156-ca4d-4e4d-b755-240907228d47",
   "metadata": {},
   "source": [
    "## GBM B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc378bf-06d0-4753-b1ed-e38dc746b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2601, number of negative: 2601000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1537\n",
      "[LightGBM] [Info] Number of data points in the train set: 2603601, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000999 -> initscore=-6.907755\n",
      "[LightGBM] [Info] Start training from score -6.907755\n"
     ]
    }
   ],
   "source": [
    "train_set = lgb.Dataset(XBPremierModelFits, YPremierModelFits)\n",
    "model_b_gbm = lgb.train(\n",
    "        params = params_b_gbm,\n",
    "        train_set=train_set   \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c193b-033b-4be9-92e0-f08d8bf11d88",
   "metadata": {},
   "source": [
    "# Save data to temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39aa997-0f28-4b00-8e10-86079b17b5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x345f90290>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write models, split characteristics to disk\n",
    "\n",
    "SplitCharacteristics.to_csv(fn_split_characteristics)\n",
    "model_a_ann.save(filepath=fn_model_a_ann)\n",
    "model_b_ann.save(filepath=fn_model_b_ann)\n",
    "model_a_gbm.save_model(fn_model_a_gbm)\n",
    "model_b_gbm.save_model(fn_model_b_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e08f4e78-a9c4-4c49-b888-429228c2e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77046/77046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 197us/step\n",
      "\u001b[1m77046/77046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 203us/step\n",
      "\u001b[1m40791/40791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 196us/step\n",
      "\u001b[1m40791/40791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 202us/step\n"
     ]
    }
   ],
   "source": [
    "y_fit_train_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTrain))\n",
    "y_fit_train_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTrain))\n",
    "\n",
    "y_fit_test_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTest))\n",
    "y_fit_test_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea552c2-b69f-4210-8c77-e540f2bb80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit_train_a_gbm = model_a_gbm.predict(XASecondaryModelTrain)\n",
    "y_fit_train_b_gbm = model_b_gbm.predict(XBSecondaryModelTrain)\n",
    "\n",
    "y_fit_test_a_gbm = model_a_gbm.predict(XASecondaryModelTest)\n",
    "y_fit_test_b_gbm = model_b_gbm.predict(XBSecondaryModelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b8b1e23-cf73-44a8-a536-6c9d98531fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSecondaryModelTrain = np.hstack([\n",
    "    XASecondaryModelTrain, \n",
    "    XBSecondaryModelTrain,\n",
    "    \n",
    "    y_fit_train_a_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_a_ann )] ).T,\n",
    "    \n",
    "    y_fit_train_b_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_b_ann )] ).T,\n",
    "\n",
    "    np.array([y_fit_train_a_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_a_gbm )] ).T,\n",
    "\n",
    "    np.array([y_fit_train_b_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_b_gbm )] ).T\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2337145c-256b-4bee-9be7-454fc6809292",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSecondaryModelTest = np.hstack([\n",
    "    XASecondaryModelTest, \n",
    "    XBSecondaryModelTest,\n",
    "    \n",
    "    y_fit_test_a_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_a_ann )] ).T,\n",
    "    \n",
    "    y_fit_test_b_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_b_ann )] ).T,\n",
    "\n",
    "    np.array([y_fit_test_a_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_a_gbm )] ).T,\n",
    "\n",
    "    np.array([y_fit_test_b_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_b_gbm )] ).T\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbeabc77-c031-4ec8-b981-383e770d9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tables to disk\n",
    "YSecondaryModelTrain.to_parquet(fn_y_train)\n",
    "YSecondaryModelTest.to_parquet(fn_y_test)\n",
    "\n",
    "pd.DataFrame(XSecondaryModelTrain).to_parquet(fn_x_train)\n",
    "pd.DataFrame(XSecondaryModelTest).to_parquet(fn_x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
