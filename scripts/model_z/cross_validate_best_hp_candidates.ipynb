{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ed92e-0b73-43b7-ba46-8186a5ccae3a",
   "metadata": {},
   "source": [
    "Stage 1 hyperparameters:\n",
    "- These are a 'fait accompli', and the hyperparameters need only be loaded.\n",
    "\n",
    "Stage 2, testable hyperparameters:\n",
    "- Load the top n number of possible hyperparameters per iteration.\n",
    "\n",
    "Load the mostly feature-engineered stage 1 X and Y files\n",
    "\n",
    "For each stage 2 hyperparameter x each fold number:\n",
    "- Split the folds into premier_model_fits (x2), secondary_model_train (x2), secondary_model_test (x1)\n",
    "- Normalize the premier_model_fits X files\n",
    "- Train all four input models on the premier_model_fits files\n",
    "- Fit a stage 2 model with the contender stage 2 hyperparameters\n",
    "- Test the loss, accuracy, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38425028-a7df-4608-8624-da6594f34d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from keras import models, layers, regularizers, optimizers, callbacks, utils, losses, metrics\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b84eb410-efd6-46d2-b97c-2c034e490309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Volumes/Extreme SSD/rematch_eia_ferc1_docker'\n",
    "dir_working_model_a_training = os.path.join(data_dir, 'working_data/model_a/model_a_training')\n",
    "dir_working_model_b_training = os.path.join(data_dir, 'working_data/model_b/model_b_training')\n",
    "\n",
    "fn_out = os.path.join(data_dir, 'working_data/model_second_stage/model_second_stage_training/second_stage_model_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492a9ca8-dd0e-4562-8c10-a909e94587a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hp2_fn = glob(os.path.join(data_dir, '**/gbm_grid*.csv'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b97319-d3d3-448f-b95e-dff47f260045",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_x_a = os.path.join(dir_working_model_a_training, 'x.parquet')\n",
    "fn_y_a = os.path.join(dir_working_model_a_training, 'y.parquet')\n",
    "fn_id = os.path.join(dir_working_model_a_training, 'id.parquet')\n",
    "\n",
    "fn_x_b = os.path.join(dir_working_model_b_training, 'x.parquet')\n",
    "fn_y_b = os.path.join(dir_working_model_b_training, 'y.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d201b9f5-b777-4db9-aaf4-44d551ad63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def np_cleaning(X):\n",
    "    X = np.clip(X, a_min=-3, a_max=3)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X\n",
    "\n",
    "def prep_x(fit_scaler, X):\n",
    "    X = fit_scaler.transform(X)\n",
    "    X = np_cleaning(X)\n",
    "    # X = convert_to_tensor(X)\n",
    "    return(X)\n",
    "\n",
    "def get_dense_desc_rank(nn):\n",
    "    # will be used for ranking y_fits\n",
    "    return( stats.rankdata(-nn, method='dense') )\n",
    "\n",
    "def define_folds(values_for_secondary_model_test):\n",
    "    # If fold f is reserved for secondary model_test, split the remaining folds\n",
    "    # half into premier_model_fits, and half into secondary_model_train. \n",
    "    # Return all values, as numpy arrays\n",
    "    fold_values = np.arange(5)\n",
    "    remaining_fold_values = np.setdiff1d(fold_values, values_for_secondary_model_test)\n",
    "    np.random.shuffle(remaining_fold_values)\n",
    "    values_for_premier_model_fits = remaining_fold_values[0:2]\n",
    "    values_for_secondary_model_train = remaining_fold_values[2:]\n",
    "    \n",
    "    print('Values for premier model fits: ', values_for_premier_model_fits)\n",
    "    print('Values for secondary model train: ', values_for_secondary_model_train)\n",
    "    print('Values for secondary model test: ', values_for_secondary_model_test)\n",
    "    \n",
    "    return values_for_premier_model_fits, values_for_secondary_model_train, np.array(values_for_secondary_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba969e9-6836-4a56-bf41-2119ab3ea7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boolean_masks_for_folds(ID, values_for_premier_model_fits, values_for_secondary_model_train, values_for_secondary_model_test):\n",
    "    is_premier_model_fits    = np.isin(element=ID.fold.values, test_elements=values_for_premier_model_fits)\n",
    "    is_secondary_model_train = np.isin(element=ID.fold.values, test_elements=values_for_secondary_model_train)\n",
    "    is_secondary_model_test  = np.isin(element=ID.fold.values, test_elements=values_for_secondary_model_test)\n",
    "    return is_premier_model_fits, is_secondary_model_train, is_secondary_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7a1fb2-c8bd-4b51-aef4-d9b1c545aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ann(params, X, Y):\n",
    "    # Can be used by model A or B\n",
    "    clear_session()\n",
    "    model_ann = models.Sequential()\n",
    "    model_ann.add(layers.Dropout(rate=params[\"dropout_1\"]))\n",
    "    model_ann.add(layers.Dense(units=int(params[\"relu_1\"]), activation='relu'))    \n",
    "    model_ann.add(layers.Dropout(rate=params[\"dropout_2\"]))\n",
    "    model_ann.add(layers.Dense(units=int(params[\"relu_2\"]), activation='relu'))   \n",
    "    model_ann.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model_ann.compile(\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryCrossentropy(),\n",
    "            metrics.BinaryAccuracy(), \n",
    "            metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    history_ann = model_ann.fit(\n",
    "        convert_to_tensor(X), Y, epochs=int(params['epochs']), batch_size=128,\n",
    "        verbose=0\n",
    "    ) \n",
    "    return model_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976ed19a-94fd-43c4-850e-12bbb0e2ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gbm(params, X, Y):\n",
    "    # Can be used by model A or B\n",
    "    train_set = lgb.Dataset(X, Y)\n",
    "    model_gbm = lgb.train(\n",
    "            params = params,\n",
    "            train_set=train_set   \n",
    "        )\n",
    "    return model_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dc0c60-ef94-4680-a6e0-daa0570e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_x(X, is_premier_model_fits, is_secondary_model_train, is_secondary_model_test):\n",
    "    # Scale X files \n",
    "    XPremierModelFits    = X.loc[is_premier_model_fits]\n",
    "    XSecondaryModelTrain = X.loc[is_secondary_model_train]\n",
    "    XSecondaryModelTest  = X.loc[is_secondary_model_test]\n",
    "    \n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler = standard_scaler.fit(XPremierModelFits)\n",
    "    \n",
    "    XPremierModelFits    = prep_x(fit_scaler=standard_scaler, X=XPremierModelFits)\n",
    "    XSecondaryModelTrain = prep_x(fit_scaler=standard_scaler, X=XSecondaryModelTrain)\n",
    "    XSecondaryModelTest  = prep_x(fit_scaler=standard_scaler, X=XSecondaryModelTest)\n",
    "    return XPremierModelFits, XSecondaryModelTrain, XSecondaryModelTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53275e9a-5dfe-4032-92a1-157b41faef3c",
   "metadata": {},
   "source": [
    "# Collect Stage 1 Hyperparameters\n",
    "\n",
    "Note that these are a 'fait accompli', and need only be read from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d068f8-aa6b-4ebf-bc26-8a284ea0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_a_ann_hp = os.path.join(data_dir, 'working_data/model_a/model_a_training/model_a_ann_hp.csv')\n",
    "hp1_a_ann = pd.read_csv(fn_model_a_ann_hp).to_dict(orient='list')\n",
    "hp1_a_ann = {k:hp1_a_ann[k][0] for k in hp1_a_ann.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff65ce5-6785-4de2-a2ca-c2c5974a1d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_a_gbm_hp = os.path.join(data_dir, 'working_data/model_a/model_a_training/model_a_gbm_hp.csv')\n",
    "hp1_a_gbm = pd.read_csv(fn_model_a_gbm_hp).to_dict(orient='list')\n",
    "hp1_a_gbm = {k:hp1_a_gbm[k][0] for k in hp1_a_gbm.keys()}\n",
    "hp1_a_gbm['metrics'] = ['binary_logloss', 'auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50bd602-cf97-495c-93d0-87b970d24aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_b_ann_hp = os.path.join(data_dir, 'working_data/model_b/model_b_training/model_b_ann_hp.csv')\n",
    "hp1_b_ann = pd.read_csv(fn_model_b_ann_hp).to_dict(orient='list')\n",
    "hp1_b_ann = {k:hp1_b_ann[k][0] for k in hp1_b_ann.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0295ca10-c14f-478f-9954-094d7ba94c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_model_b_gbm_hp = os.path.join(data_dir, 'working_data/model_b/model_b_training/model_b_gbm_hp.csv')\n",
    "hp1_b_gbm = pd.read_csv(fn_model_b_gbm_hp).to_dict(orient='list')\n",
    "hp1_b_gbm = {k:hp1_b_gbm[k][0] for k in hp1_b_gbm.keys()}\n",
    "hp1_b_gbm['metrics'] = ['binary_logloss', 'auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7131519-9746-4a4c-b866-42098dd69e1d",
   "metadata": {},
   "source": [
    "# Collect Stage 2 Contender Hyperparameters\n",
    "\n",
    "Filter to the top n contenders per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de55bf23-b4de-4410-af82-3f5a0e027c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinedHP2 = pd.DataFrame()\n",
    "\n",
    "for fn in list_hp2_fn:\n",
    "    HP2 = pd.read_csv(fn)\n",
    "    HP2['fn'] = fn\n",
    "    JoinedHP2 = pd.concat([JoinedHP2, HP2])\n",
    "\n",
    "mask_is_hp2_contender = JoinedHP2['rank'] <= 15  # NB this is what the user can change to test more possible hyperparameters! 0 is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50deefdf-b751-4b0b-a8fb-40a211fc7799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold_num</th>\n",
       "      <th>num_trees</th>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>fn</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>162</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>843</td>\n",
       "      <td>129</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>861</td>\n",
       "      <td>166</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>162</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>925</td>\n",
       "      <td>142</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>770</td>\n",
       "      <td>154</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>4</td>\n",
       "      <td>615</td>\n",
       "      <td>176</td>\n",
       "      <td>0.011748</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>4</td>\n",
       "      <td>804</td>\n",
       "      <td>166</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>4</td>\n",
       "      <td>798</td>\n",
       "      <td>161</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>4</td>\n",
       "      <td>688</td>\n",
       "      <td>156</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>/Volumes/Extreme SSD/rematch_eia_ferc1_docker/...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fold_num  num_trees  min_data_in_leaf  learning_rate  \\\n",
       "0           0        580               162       0.013730   \n",
       "1           0        843               129       0.012509   \n",
       "2           0        861               166       0.012041   \n",
       "3           0        901               162       0.011118   \n",
       "4           0        925               142       0.012750   \n",
       "..        ...        ...               ...            ...   \n",
       "235         4        770               154       0.009563   \n",
       "236         4        615               176       0.011748   \n",
       "237         4        804               166       0.012381   \n",
       "238         4        798               161       0.012806   \n",
       "239         4        688               156       0.012814   \n",
       "\n",
       "                                                    fn  rank  \n",
       "0    /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...     0  \n",
       "1    /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...     1  \n",
       "2    /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...     2  \n",
       "3    /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...     3  \n",
       "4    /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...     4  \n",
       "..                                                 ...   ...  \n",
       "235  /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...    11  \n",
       "236  /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...    12  \n",
       "237  /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...    13  \n",
       "238  /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...    14  \n",
       "239  /Volumes/Extreme SSD/rematch_eia_ferc1_docker/...    15  \n",
       "\n",
       "[240 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContenderHP2 = JoinedHP2.loc[mask_is_hp2_contender, ['config/num_trees', 'config/min_data_in_leaf', 'config/learning_rate', 'fn', 'rank']].copy()\n",
    "ContenderHP2.reset_index(inplace=True, drop=True)\n",
    "ContenderHP2.rename(columns={'config/num_trees': 'num_trees', 'config/min_data_in_leaf': 'min_data_in_leaf', 'config/learning_rate': 'learning_rate'}, inplace=True)\n",
    "ContenderHP2 = pd.DataFrame({'fold_num':np.arange(5)}).merge(ContenderHP2, how='cross')\n",
    "ContenderHP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c21d3e6-2fce-4074-83e0-5a6c115a3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fold_num': 0, 'num_trees': 580, 'min_data_in_leaf': 162, 'learning_rate': 0.0137300158848056}\n"
     ]
    }
   ],
   "source": [
    "stage_2_param_dict = ContenderHP2[['fold_num', 'num_trees', 'min_data_in_leaf', 'learning_rate']].to_dict('index')\n",
    "print(stage_2_param_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66136f3a-f1d6-43f1-b3bc-de85adbdb64f",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b1a7e03-bd54-4dd3-8eda-f396546b6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = pd.read_parquet(fn_x_a)\n",
    "X_b = pd.read_parquet(fn_x_b)\n",
    "Y = pd.read_parquet(fn_y_a)\n",
    "ID = pd.read_parquet(fn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fcaa7-88f3-4795-ad2d-e02a92847738",
   "metadata": {},
   "source": [
    "# Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a8d71-5864-483e-9be8-17880ae1d8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00ca886a8534d86b8a9c497cfc17606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for premier model fits:  [3 4]\n",
      "Values for secondary model train:  [2 1]\n",
      "Values for secondary model test:  0\n"
     ]
    }
   ],
   "source": [
    "collected_results = []\n",
    "for i in tqdm(stage_2_param_dict.keys()):\n",
    "    \n",
    "    params = stage_2_param_dict[i]\n",
    "    \n",
    "    # Note which fold we're on, and divvy up the data into test/train bits, based on that\n",
    "    values_for_premier_model_fits, values_for_secondary_model_train, values_for_secondary_model_test = define_folds( params['fold_num'] )\n",
    "\n",
    "    is_premier_model_fits, is_secondary_model_train, is_secondary_model_test = get_boolean_masks_for_folds(\n",
    "        ID=ID, \n",
    "        values_for_premier_model_fits=values_for_premier_model_fits, \n",
    "        values_for_secondary_model_train=values_for_secondary_model_train, \n",
    "        values_for_secondary_model_test=values_for_secondary_model_test\n",
    "    )\n",
    "\n",
    "    # Y, ID\n",
    "    YPremierModelFits = Y.loc[is_premier_model_fits]\n",
    "    YSecondaryModelTrain = Y.loc[is_secondary_model_train]\n",
    "    YSecondaryModelTest = Y.loc[is_secondary_model_test]\n",
    "    \n",
    "    IDSecondaryModelTest = ID.loc[is_secondary_model_test]\n",
    "\n",
    "    # XA\n",
    "    XAPremierModelFits, XASecondaryModelTrain, XASecondaryModelTest = clean_x(\n",
    "        X=X_a, \n",
    "        is_premier_model_fits=is_premier_model_fits, \n",
    "        is_secondary_model_train=is_secondary_model_train, \n",
    "        is_secondary_model_test=is_secondary_model_test\n",
    "    )\n",
    "\n",
    "    # XB\n",
    "    XBPremierModelFits, XBSecondaryModelTrain, XBSecondaryModelTest = clean_x(\n",
    "        X=X_b, \n",
    "        is_premier_model_fits=is_premier_model_fits, \n",
    "        is_secondary_model_train=is_secondary_model_train, \n",
    "        is_secondary_model_test=is_secondary_model_test\n",
    "    )\n",
    "\n",
    "    # Fit models for stage 1, get y_fit\n",
    "    model_a_ann = fit_ann(params=hp1_a_ann, X=XAPremierModelFits, Y=YPremierModelFits)\n",
    "    model_a_gbm = fit_gbm(params=hp1_a_gbm, X=XAPremierModelFits, Y=YPremierModelFits)\n",
    "    model_b_ann = fit_ann(params=hp1_b_ann, X=XBPremierModelFits, Y=YPremierModelFits)\n",
    "    model_b_gbm = fit_gbm(params=hp1_b_gbm, X=XBPremierModelFits, Y=YPremierModelFits)\n",
    "\n",
    "    # ANN, stage 2 Train\n",
    "    y_fit_train_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTrain))\n",
    "    y_fit_train_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTrain))\n",
    "\n",
    "    # ANN, stage 2 Test\n",
    "    y_fit_test_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTest))\n",
    "    y_fit_test_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTest))\n",
    "\n",
    "    # GBM, stage 2 Train\n",
    "    y_fit_train_a_gbm = model_a_gbm.predict(XASecondaryModelTrain)\n",
    "    y_fit_train_b_gbm = model_b_gbm.predict(XBSecondaryModelTrain)\n",
    "\n",
    "    # GBM, stage 2 Test\n",
    "    y_fit_test_a_gbm = model_a_gbm.predict(XASecondaryModelTest)\n",
    "    y_fit_test_b_gbm = model_b_gbm.predict(XBSecondaryModelTest)\n",
    "\n",
    "    # Collect the above into something the 2nd stage model can use\n",
    "    XSecondaryModelTrain = np.hstack([\n",
    "        XASecondaryModelTrain, \n",
    "        XBSecondaryModelTrain,\n",
    "        \n",
    "        y_fit_train_a_ann,\n",
    "        np.array( [get_dense_desc_rank( y_fit_train_a_ann )] ).T,\n",
    "        \n",
    "        y_fit_train_b_ann,\n",
    "        np.array( [get_dense_desc_rank( y_fit_train_b_ann )] ).T,\n",
    "    \n",
    "        np.array([y_fit_train_a_gbm]).T,\n",
    "        np.array( [get_dense_desc_rank( y_fit_train_a_gbm )] ).T,\n",
    "    \n",
    "        np.array([y_fit_train_b_gbm]).T,\n",
    "        np.array( [get_dense_desc_rank( y_fit_train_b_gbm )] ).T\n",
    "    ])\n",
    "    \n",
    "    XSecondaryModelTest = np.hstack([\n",
    "        XASecondaryModelTest, \n",
    "        XBSecondaryModelTest,\n",
    "        \n",
    "        y_fit_test_a_ann,\n",
    "        np.array( [get_dense_desc_rank( y_fit_test_a_ann )] ).T,\n",
    "        \n",
    "        y_fit_test_b_ann,\n",
    "        np.array( [get_dense_desc_rank( y_fit_test_b_ann )] ).T,\n",
    "    \n",
    "        np.array([y_fit_test_a_gbm]).T,\n",
    "        np.array( [get_dense_desc_rank( y_fit_test_a_gbm )] ).T,\n",
    "    \n",
    "        np.array([y_fit_test_b_gbm]).T,\n",
    "        np.array( [get_dense_desc_rank( y_fit_test_b_gbm )] ).T\n",
    "    ])\n",
    "\n",
    "    # def fit_mod(stage_2_params, XTrain, XTest, YTrain, YTest):\n",
    "    \n",
    "    XTrain = XSecondaryModelTrain\n",
    "    XTest = XSecondaryModelTest\n",
    "    YTrain = YSecondaryModelTrain\n",
    "    YTest = YSecondaryModelTest\n",
    "    \n",
    "    \n",
    "    # Package in training and testing objects\n",
    "    train_set = lgb.Dataset(XTrain, YTrain)\n",
    "    test_set  = lgb.Dataset(XTest,  YTest)\n",
    "    \n",
    "    # Model\n",
    "    gbm = lgb.train(\n",
    "        params,\n",
    "        train_set\n",
    "        # valid_sets=[test_set]    \n",
    "    )\n",
    "    y_fit = gbm.predict(XTest)\n",
    "\n",
    "    # Goodness of fit\n",
    "    Framework = IDSecondaryModelTest[['record_id_ferc1']].copy()\n",
    "    Framework['y_fit'] = y_fit\n",
    "    Framework['groupwise_max_y_fit'] = Framework.groupby('record_id_ferc1')['y_fit'].transform('max')\n",
    "    Framework['y_fit_adj'] = Framework['y_fit'] == Framework['groupwise_max_y_fit']\n",
    "    \n",
    "    gof_dict = {\n",
    "        'precision' : sklearn_metrics.precision_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "        'recall' : sklearn_metrics.recall_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "        'log_loss' : sklearn_metrics.log_loss(YTest.values, y_fit),\n",
    "        'roc_auc' : sklearn_metrics.roc_auc_score(YTest.values, y_fit)\n",
    "    }\n",
    "\n",
    "    results = stage_2_param_dict[i] | gof_dict\n",
    "    collected_results.append(results)\n",
    "\n",
    "CollectedResults = pd.concat([pd.DataFrame(dict, index=[1]) for dict in collected_results])\n",
    "CollectedResults.reset_index(drop=True, inplace=True)\n",
    "CollectedResults.to_csv(fn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df424c59-059a-42c3-bc84-ddc4ebeb1073",
   "metadata": {},
   "source": [
    "## Split data: iterate from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2ea336-a6d8-4d79-bd55-c4e0da930cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "fold_num = stage_2_param_dict[i]['fold_num']\n",
    "# num_trees = 580\n",
    "# min_data_in_leaf = 162\n",
    "# learning_rate = 0.013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "426cf25a-c78d-4bed-8eff-a8fc6893a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for premier model fits:  [3 4]\n",
      "Values for secondary model train:  [2 1]\n",
      "Values for secondary model test:  0\n"
     ]
    }
   ],
   "source": [
    "values_for_premier_model_fits, values_for_secondary_model_train, values_for_secondary_model_test = define_folds(fold_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06586735-a6b7-4e2d-acc9-720f438a97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_premier_model_fits, is_secondary_model_train, is_secondary_model_test = get_boolean_masks_for_folds(\n",
    "    ID=ID, \n",
    "    values_for_premier_model_fits=values_for_premier_model_fits, \n",
    "    values_for_secondary_model_train=values_for_secondary_model_train, \n",
    "    values_for_secondary_model_test=values_for_secondary_model_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac7e9af-1c71-48d0-9705-7efe268e097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "YPremierModelFits = Y.loc[is_premier_model_fits]\n",
    "YSecondaryModelTrain = Y.loc[is_secondary_model_train]\n",
    "YSecondaryModelTest = Y.loc[is_secondary_model_test]\n",
    "\n",
    "IDSecondaryModelTest = ID.loc[is_secondary_model_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a0016c8-c80a-4dc9-9fa6-07a6677b47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAPremierModelFits, XASecondaryModelTrain, XASecondaryModelTest = clean_x(\n",
    "    X=X_a, \n",
    "    is_premier_model_fits=is_premier_model_fits, \n",
    "    is_secondary_model_train=is_secondary_model_train, \n",
    "    is_secondary_model_test=is_secondary_model_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf167af6-9284-4023-a374-114e29caee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XBPremierModelFits, XBSecondaryModelTrain, XBSecondaryModelTest = clean_x(\n",
    "    X=X_b, \n",
    "    is_premier_model_fits=is_premier_model_fits, \n",
    "    is_secondary_model_train=is_secondary_model_train, \n",
    "    is_secondary_model_test=is_secondary_model_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a05e985e-6f41-4165-98f4-260ec0066bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_ann = fit_ann(params=hp1_a_ann, X=XAPremierModelFits, Y=YPremierModelFits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38d9b665-1bf9-4e5f-b4ff-ae279d8cac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "model_a_gbm = fit_gbm(params=hp1_a_gbm, X=XAPremierModelFits, Y=YPremierModelFits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5696092d-9b5e-4a94-9db5-c4ae09d5f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_ann = fit_ann(params=hp1_b_ann, X=XBPremierModelFits, Y=YPremierModelFits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47553822-f49f-43aa-815a-b6994816e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "model_b_gbm = fit_gbm(params=hp1_b_gbm, X=XBPremierModelFits, Y=YPremierModelFits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "129a6273-5c23-4d96-9897-5f4438967f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81019/81019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 205us/step\n",
      "\u001b[1m81019/81019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 206us/step\n",
      "\u001b[1m38946/38946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201us/step\n",
      "\u001b[1m38946/38946\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 207us/step\n"
     ]
    }
   ],
   "source": [
    "y_fit_train_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTrain))\n",
    "y_fit_train_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTrain))\n",
    "\n",
    "y_fit_test_a_ann = model_a_ann.predict(convert_to_tensor(XASecondaryModelTest))\n",
    "y_fit_test_b_ann = model_b_ann.predict(convert_to_tensor(XBSecondaryModelTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "134c1821-0018-4803-88af-6a0d643f8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit_train_a_gbm = model_a_gbm.predict(XASecondaryModelTrain)\n",
    "y_fit_train_b_gbm = model_b_gbm.predict(XBSecondaryModelTrain)\n",
    "\n",
    "y_fit_test_a_gbm = model_a_gbm.predict(XASecondaryModelTest)\n",
    "y_fit_test_b_gbm = model_b_gbm.predict(XBSecondaryModelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ea14a0a-cac6-485a-9386-5068f642fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSecondaryModelTrain = np.hstack([\n",
    "    XASecondaryModelTrain, \n",
    "    XBSecondaryModelTrain,\n",
    "    \n",
    "    y_fit_train_a_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_a_ann )] ).T,\n",
    "    \n",
    "    y_fit_train_b_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_b_ann )] ).T,\n",
    "\n",
    "    np.array([y_fit_train_a_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_a_gbm )] ).T,\n",
    "\n",
    "    np.array([y_fit_train_b_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_train_b_gbm )] ).T\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4458a0c-beb2-4d1b-91c5-65bd528b832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSecondaryModelTest = np.hstack([\n",
    "    XASecondaryModelTest, \n",
    "    XBSecondaryModelTest,\n",
    "    \n",
    "    y_fit_test_a_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_a_ann )] ).T,\n",
    "    \n",
    "    y_fit_test_b_ann,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_b_ann )] ).T,\n",
    "\n",
    "    np.array([y_fit_test_a_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_a_gbm )] ).T,\n",
    "\n",
    "    np.array([y_fit_test_b_gbm]).T,\n",
    "    np.array( [get_dense_desc_rank( y_fit_test_b_gbm )] ).T\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1668f5e-033e-4385-bb98-98d2110f989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: fold_num\n",
      "[LightGBM] [Warning] Unknown parameter: fold_num\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.301950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6421\n",
      "[LightGBM] [Info] Number of data points in the train set: 2592590, number of used features: 141\n",
      "[LightGBM] [Info] Start training from score 0.000999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# def fit_mod(stage_2_params, XTrain, XTest, YTrain, YTest):\n",
    "stage_2_params = stage_2_param_dict[0]\n",
    "XTrain = XSecondaryModelTrain\n",
    "XTest = XSecondaryModelTest\n",
    "YTrain = YSecondaryModelTrain\n",
    "YTest = YSecondaryModelTest\n",
    "\n",
    "\n",
    "# Package in training and testing objects\n",
    "train_set = lgb.Dataset(XTrain, YTrain)\n",
    "test_set  = lgb.Dataset(XTest,  YTest)\n",
    "\n",
    "# Model\n",
    "gbm = lgb.train(\n",
    "    stage_2_params,\n",
    "    train_set\n",
    "    # valid_sets=[test_set]    \n",
    ")\n",
    "y_fit = gbm.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fc7aabb-55ef-4dfa-b2a6-d96a8ecc19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Framework = IDSecondaryModelTest[['record_id_ferc1']].copy()\n",
    "Framework['y_fit'] = y_fit\n",
    "Framework['groupwise_max_y_fit'] = Framework.groupby('record_id_ferc1')['y_fit'].transform('max')\n",
    "Framework['y_fit_adj'] = Framework['y_fit'] == Framework['groupwise_max_y_fit']\n",
    "\n",
    "gof_dict = {\n",
    "    'precision' : sklearn_metrics.precision_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "    'recall' : sklearn_metrics.recall_score(YTest.values, Framework['y_fit_adj'].values*1),\n",
    "    'log_loss' : sklearn_metrics.log_loss(YTest.values, y_fit),\n",
    "    'roc_auc' : sklearn_metrics.roc_auc_score(YTest.values, y_fit)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d11ce8b-0329-4d9e-bed6-83c72221cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_num': 0,\n",
       " 'num_trees': 580,\n",
       " 'min_data_in_leaf': 162,\n",
       " 'learning_rate': 0.0137300158848056,\n",
       " 'precision': 0.9840383080606544,\n",
       " 'recall': 0.9903614457831326,\n",
       " 'log_loss': 0.0003007053699996441,\n",
       " 'roc_auc': 0.9975861479653554}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = stage_2_param_dict[i] | gof_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
